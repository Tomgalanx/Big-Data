# -*- coding: utf-8 -*-
"""Graphe

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12KfZViCfwdwFv6Nyc5W6cS9C3UGHn7mH
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz
!tar xf spark-3.0.1-bin-hadoop3.2.tgz
!pip install -q findspark

!ls -a

#!spark-3.0.1-bin-hadoop3.2/bin/pyspark
!spark-3.0.1-bin-hadoop3.2/bin/pyspark --packages graphframes:graphframes:0.8.1-spark2.4-s_2.11

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.1-bin-hadoop3.2"

!pip install pyspark

import pyspark
print(pyspark.__version__)

!uname -a
!lsb_release

from google.colab import drive
drive.mount('/content/drive')

!pip install graphframes

#Louarrat Wafaa
#Thomas Gerphanion
#GRAPHE 1

import findspark
findspark.init()

from pyspark.sql import Row
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark.sql.types import *
import pandas as pd
from graphframes import *
from pyspark.sql.functions import *
from graphframes.lib import AggregateMessages as AM

#Creation de SparkSession
spark = SparkSession.builder.appName("GraphFrames").config("spark.some.config.option", "local").getOrCreate()

#Creation du sparkContext
sc = spark.sparkContext

#Definir sqlContext
sqlContext = SQLContext(sc)

#Creation de pyspark DataFrame
ratings = spark.createDataFrame(
    sc.textFile("/home/echantillon-flows.txt").map(lambda l: l.split(',')),
    ["temps","duree","ordinateur_source","port_source","ordinateur_de_destination","port_de_destination","protocole","nombre_de_paquets","nombre_doctets"])

#Convertir les colonnes du dataFrame ratings aux types demandes dans le sujet
ratings = ratings.withColumn("temps", ratings["temps"].cast("Timestamp")).withColumn("duree", ratings["duree"].cast("Integer")).withColumn("ordinateur_source", ratings["ordinateur_source"].cast("String")).withColumn("port_source", ratings["port_source"].cast("String")).withColumn("ordinateur_de_destination", ratings["ordinateur_de_destination"].cast("String")).withColumn("port_de_destination", ratings["port_de_destination"].cast("String")).withColumn("protocole", ratings["protocole"].cast("String")).withColumn("nombre_de_paquets", ratings["nombre_de_paquets"].cast("Integer")).withColumn("nombre_doctets", ratings["nombre_doctets"].cast("Integer"))
ratings.registerTempTable("ratings")

#Affichage des types des colonnes du dataFrame ratings
ratings.dtypes

#Creation du dataFrame final_df
final_df = sqlContext.sql("select * from ratings");
#Affichage du final_df
final_df.show(10)

#Utilisation de la requete sql pour calculer le count, mean, sum, stddev, min, max
final_df = sqlContext.sql("SELECT ordinateur_source, ordinateur_de_destination, count(*) as count, mean(nombre_de_paquets) as mean, sum(nombre_de_paquets) as sum, stddev(nombre_de_paquets) as stddev, min(nombre_de_paquets) as min, max(nombre_de_paquets) as max FROM ratings GROUP BY ordinateur_source, ordinateur_de_destination")
#Affichage du final_df
final_df.show()
#final_df.dtypes

#Creation des sommets
v = ratings.select('ordinateur_source').union(ratings.select('ordinateur_de_destination'))
v = ratings.selectExpr("ordinateur_source as ordinateur")
v.show()

#Creation des arcs
e = final_df.withColumn("connection", lit("connected"))
e.show()


#Creation du GraphFrame g
g = GraphFrame(v, e)

#InDegrees
nb_arc_entrants = g.inDegrees()
#Affichage des InDegrees
nb_arc_entrants.show()
#OutDegrees
nb_arc_sortants = g.outDegrees()
#Affichage des OutDegrees
nb_arc_sortants.show()

#Calcule du "sum" des param√®tres de arcs entrant et sortant d'un sommet
#Somme des count des nombre de paquets
msgToSrc = AM.ordinateur["count"]
msgToDst = AM.ordinateur["count"]
agg = g.aggregateMessages(
    sum(AM.msg).alias("summedCount"),
    sendToSrc=msgToSrc,
    sendToDst=msgToDst)
agg.show()

#Somme des mean des nombre de paquets
msgToSrc = AM.ordinateur["mean"]
msgToDst = AM.ordinateur["mean"]
agg = g.aggregateMessages(
    sum(AM.msg).alias("summedMean"),
    sendToSrc=msgToSrc,
    sendToDst=msgToDst)
agg.show()

#Somme des stddev des nombre de paquets
msgToSrc = AM.ordinateur["stddev"]
msgToDst = AM.ordinateur["stddev"]
agg = g.aggregateMessages(
    sum(AM.msg).alias("summedStddev"),
    sendToSrc=msgToSrc,
    sendToDst=msgToDst)
agg.show()

#Somme des min des nombre de paquets
msgToSrc = AM.ordinateur["Min"]
msgToDst = AM.ordinateur["Min"]
agg = g.aggregateMessages(
    sum(AM.msg).alias("summedMin"),
    sendToSrc=msgToSrc,
    sendToDst=msgToDst)
agg.show()

#Somme des max des nombre de paquets
msgToSrc = AM.ordinateur["Max"]
msgToDst = AM.ordinateur["Max"]
agg = g.aggregateMessages(
    sum(AM.msg).alias("summedMax"),
    sendToSrc=msgToSrc,
    sendToDst=msgToDst)
agg.show()


#GRAPHE 2

#Creation des sommets
v = ratings.select('ordinateur_source').union(ratings.select('ordinateur_de_destination'))
v = ratings.selectExpr("ordinateur_source as ordinateur")
v.show()

#Creation des arcs
e = ratings.withColumn("connection", lit("connected"))
e.show()

#Creation du GraphFrame g
g = GraphFrame(v, e)

#InDegrees
nb_arc_entrants = g.inDegrees()
#Affichage #InDegrees
nb_arc_entrants.show()
#OutDegrees
nb_arc_sortants = g.outDegrees()
#Affichage OutDegrees
nb_arc_sortants.show()


#Pour chaque sommet, calcule du sum, mean, min, max du nombre de paquets des arcs entrant et sortant d'un sommet
msgToSrc = AM.ordinateur["nombre_de_paquets"]
msgToDst = AM.ordinateur["nombre_de_paquets"]
agg = g.aggregateMessages(
    sum(AM.msg).alias("sumNombre_de_paquets"),
    mean(AM.msg).alias("meanNombre_de_paquets"),
    min(AM.msg).alias("minNombre_de_paquets"),
    max(AM.msg).alias("maxNombre_de_paquets"),
    sendToSrc=msgToSrc,
    sendToDst=msgToDst)
agg.show()